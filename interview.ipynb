{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4g9sUAfes4oV83V11UwGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyg00/itp303_full_stack_web_project/blob/master/interview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWgNiHR9i0ei",
        "outputId": "a2902b60-8940-4fb3-8646-ccc4fe235aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3567, 0.3288, 0.3145],\n",
            "         [0.3314, 0.3258, 0.3429],\n",
            "         [0.3077, 0.3274, 0.3649]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[ 0.0036,  0.4110, -0.3547],\n",
            "         [ 0.0031,  0.3927, -0.3383],\n",
            "         [ 0.0032,  0.3774, -0.3216]]], grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "# self-attention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "# ref: https://github.com/zxuu/Self-Attention/blob/main/Decoder/My_Decoder.py， https://mdnice.com/writing/fc0b920d4ca84837a5712df1a46865d2\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, d_qk, d_v, mask=None):\n",
        "    # super(SelfAttention, self).__init__()\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_qk = d_qk\n",
        "    self.d_v = d_v\n",
        "    self.mask = mask\n",
        "    self.initialize_Q = nn.Linear(d_model, d_qk, bias=False)\n",
        "    self.initialize_K = nn.Linear(d_model, d_qk, bias=False)\n",
        "    self.initialize_V = nn.Linear(d_model, d_v, bias=False)\n",
        "    self._norm_fact = 1 / math.sqrt(self.d_qk) # or d_k(model) = k.size()[-1], or size(-1)\n",
        "  def forward(self, x):\n",
        "    # x: [batch, seq_len, d_model]\n",
        "    # linear transformation of x for Q,K,V in self attention\n",
        "    Q = self.initialize_Q(x) # [batch, seq_len, d_qk]\n",
        "    K = self.initialize_K(x) # [batch, seq_len, d_qk]\n",
        "    V = self.initialize_V(x) # [batch, seq_len, d_v]\n",
        "    # print(Q.size())\n",
        "    # print(V.size())\n",
        "    attn_logits = torch.matmul(Q, K.transpose(-2,-1)) * self._norm_fact # [batch, seq_len, d_qk] => [batch, seq_len, seq_len]\n",
        "    if self.mask is not None:\n",
        "      print(attn_logits.size())\n",
        "      attn_logits = attn_logits.masked_fill(self.mask == 0, -9e15)\n",
        "    attn_scores = torch.softmax(attn_logits, dim=-1) # [batch, seq_len, seq_len]\n",
        "    attn_values = torch.matmul(attn_scores, V) # [batch, seq_len, d_v]\n",
        "    return attn_values, attn_scores\n",
        "batch_size = 3\n",
        "d_model = 6\n",
        "seq_len = 4\n",
        "d_v = 8\n",
        "d_qk = 7\n",
        "# x = [[[ 0.1532, -0.0044,  0.6197,  0.6332, -1.1813,  0.3393],\n",
        "#          [ 0.4235, -0.1569,  0.1241,  0.0882,  0.2117,  1.0789],\n",
        "#          [-0.1987, -0.2671, -0.2119, -0.3006, -0.1815,  0.0309],\n",
        "#          [ 0.5800, -0.0540, -0.3753, -0.3589,  0.0209,  0.0198]],\n",
        "\n",
        "#         [[ 0.6163, -0.1154, -0.4507,  0.0220, -0.2439, -0.2730],\n",
        "#          [-0.2422, -0.2329,  0.4126, -0.1035, -0.7506,  0.6158],\n",
        "#          [-1.1735,  0.1261,  0.1183, -0.5666, -0.0784,  1.1600],\n",
        "#          [ 1.4221,  0.5371,  1.0822,  0.3618, -1.1405, -0.2908]],\n",
        "\n",
        "#         [[-1.4205, -0.3030,  0.6546,  0.1172,  0.0054,  0.2997],\n",
        "#          [ 0.1374,  0.1845,  0.0117, -0.2321, -0.5516, -0.0238],\n",
        "#          [-0.2277,  0.2537, -0.0104, -0.1850,  0.3958,  0.7365],\n",
        "#          [ 0.4063, -0.1534,  0.2482,  0.2756,  0.7651,  1.8565]]]\n",
        "batch_size = 1\n",
        "d_model = 5\n",
        "seq_len = 3 #一句话三个词 每个token dim是2, 一个batch 1个句子\n",
        "d_qk = 3\n",
        "d_v = 3\n",
        "mask = [[[1,1,1],\n",
        "        [1,0, 0],\n",
        "         [0,0,1]]]\n",
        "# x = torch.tensor(x)\n",
        "mask = torch.tensor(mask)\n",
        "x_gen = torch.randn(batch_size, seq_len, d_model)\n",
        "attention = SelfAttention(d_model, d_qk, d_v,mask=None)\n",
        "res, w = attention(x_gen)\n",
        "print(w)\n",
        "print(res)\n",
        "print(res.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiheadAttention(SelfAttention):\n",
        "import torch.nn.functional as F\n",
        "class MultiheadAttention(nn.Module):\n",
        "    # input : batch_size * seq_len * input_dim\n",
        "    # q : batch_size * input_dim * dim_k\n",
        "    # k : batch_size * input_dim * dim_k\n",
        "    # v : batch_size * input_dim * dim_v\n",
        "  def __init__(self, n_heads, d_model, q, k, v, mask=None, dropout=None):\n",
        "    # super().__init__(d_model, d_qk, d_v)\n",
        "    # self.n_heads = n_heads\n",
        "    # d_k = d_v = d_model / n_heads\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.d_qk = d_model // n_heads # qkv 同维度\n",
        "    self.d_v = d_model // n_heads\n",
        "    self.mask = mask\n",
        "    self.dropout = dropout\n",
        "    # self.q = q\n",
        "    # self.k = k\n",
        "    # self.v = v\n",
        "    self.Q = nn.Linear(d_model, d_model, bias=False)(q)\n",
        "    self.K = nn.Linear(d_model, d_model, bias=False)(k)\n",
        "    self.V = nn.Linear(d_model, d_model, bias=False)(v)\n",
        "\n",
        "  def self_attention(self, q, k, v, dropout=None, mask=None):\n",
        "    d_k = k.size()[-1] # d_k=单词的embedding长度\n",
        "    # print(d_k == self.d_qk)\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k) #bmm\n",
        "    if mask is not None: # softmax 前 mask\n",
        "      attn_logits = attn_logits.masked_fill(mask == 0, -1e9)\n",
        "      print('masked:', attn_logits)\n",
        "    attn = F.softmax(attn_logits, dim=-1) # torch.softmax also works, this one is module. F one is a function\n",
        "    if dropout is not None:\n",
        "      attn = dropout(attn)\n",
        "    scores = torch.matmul(attn, v)\n",
        "    return scores, attn\n",
        "\n",
        "  def forward(self):\n",
        "    '''\n",
        "    q,k,v linear to Q, K, V: [batch_size, seq_len, d_model]\n",
        "    '''\n",
        "    batch_size = self.q.size()[0] # or size(0)\n",
        "    # [b, seq_len, d_model] => [b, seq_len, heads, d_model // heads(head size)] => [b, heads, seq_len, split_emb_size]\n",
        "    mq = self.Q.view(batch_size, -1, self.n_heads, self.d_qk).transpose(1, 2) # split embed vertically for multihead purpose, in this case view == reshape since store contiguously\n",
        "    mk = self.K.view(batch_size, -1, self.n_heads, self.d_qk).transpose(1, 2)\n",
        "    mv = self.V.view(batch_size, -1, self.n_heads, self.d_qk).transpose(1, 2)\n",
        "    if self.mask is not None:\n",
        "    # 多头注意力机制的线性变换层是4维，是把query[batch, frame_num or seq_len, d_model]变成[batch, -1, head, d_k(d_qk)]\n",
        "    # 再1，2维交换变成[batch, head, -1, d_k], 所以mask要在第一维添加一维，与后面的self attention计算维度一样\n",
        "      self.mask = self.mask.unsqueeze(1)\n",
        "    '''\n",
        "    # print(mq)\n",
        "    # print(self.Q.size(), mq.size())\n",
        "    # # mq = self.Q.reshape(batch_size, self.n_heads, self.d_qk).transpose(1, 2)\n",
        "    '''\n",
        "    x, attn = self.self_attention(mq, mk, mv) # scores: [b, heads, seq_len, split_emb_size] weights: [b, heads, seq_len, seq_len]\n",
        "    # 变为三维， 或者说是concat head(拼接各个head-attention的结果)\n",
        "    x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_qk) # [b, seq_len, d_model]\n",
        "    # fc layer\n",
        "    x = nn.Linear(d_model, d_model)(x) # [b, seq_len, d_model]\n",
        "    return x\n",
        "n_heads = 3\n",
        "seq_len = 5\n",
        "d_model = 12\n",
        "batch_size = 1\n",
        "q = torch.randn(batch_size, seq_len, d_model)\n",
        "k = torch.randn(batch_size, seq_len, d_model)\n",
        "v = torch.randn(batch_size, seq_len, d_model)\n",
        "multi = MultiheadAttention(n_heads, d_model, q,k,v)\n",
        "res = multi()\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_1vrO1Lx_H4",
        "outputId": "2eee9176-868c-410c-83b1-4927f2d86fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2324,  0.2188,  0.1741, -0.3829, -0.3040,  0.1120, -0.0527,\n",
            "          -0.1659,  0.0654, -0.2432,  0.1959,  0.0615],\n",
            "         [ 0.2558,  0.2721,  0.0776, -0.4345, -0.2776,  0.1389, -0.0541,\n",
            "          -0.1901,  0.0287, -0.2396,  0.1855,  0.0797],\n",
            "         [ 0.2673,  0.2859,  0.0894, -0.4039, -0.2944,  0.1376, -0.0608,\n",
            "          -0.1687,  0.0756, -0.2283,  0.1990,  0.0636],\n",
            "         [ 0.2553,  0.2405,  0.1331, -0.3805, -0.2979,  0.1422, -0.0395,\n",
            "          -0.1742,  0.0645, -0.2247,  0.2040,  0.0701],\n",
            "         [ 0.2560,  0.2950,  0.1015, -0.3900, -0.2904,  0.1141, -0.0682,\n",
            "          -0.1585,  0.0956, -0.2379,  0.1998,  0.0600]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch norm 2D and 1D\n",
        "import numpy as np\n",
        "class BatchNorm2D():\n",
        "    gamma, beta = 1, 0  # 缩放因子γ和平移因子β，能训练的参数\n",
        "    def __init__(self, channel, momentum=0.1, eps=1e-5):\n",
        "        self.running_mean = np.zeros(channel) # 用于测试时\n",
        "        self.running_var = np.ones(channel)   # 同上\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps                        # 接近于0的数，用于避免分母为0\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input.shape: (B, C, H, W)\n",
        "        len_ch = input.size(1)\n",
        "        output = np.zeros(input.size())\n",
        "\n",
        "        for i in range(len_ch):\n",
        "            in_ch = input[:, i, :, :]\n",
        "            total_elem = in_ch.numel()\n",
        "\n",
        "            if self.training:\n",
        "                # 计算均值和方差，并归一化\n",
        "                mean = in_ch.sum() / total_elem\n",
        "                var = ((in_ch - mean) ** 2).sum() / total_elem\n",
        "                out_ch = (in_ch - mean) / (var + self.eps) ** 0.5  # 归一化\n",
        "\n",
        "                # 更新参数\n",
        "                var_unbiased = ((in_ch - mean) ** 2).sum() / (total_elem - 1)\n",
        "                self.running_mean[i] = self.running_mean[i] * (1 - self.momentum) + mean * self.momentum\n",
        "                self.running_var[i] = self.running_var[i] * (1 - self.momentum) + var_unbiased * self.momentum\n",
        "            else:\n",
        "                out_ch = (in_ch - self.running_mean[i]) / (self.running_var[i] + self.eps) ** 0.5\n",
        "\n",
        "\n",
        "            out_ch = self.gamma * out_ch + self.beta  # 缩放平移\n",
        "            output[:, i, :, :] = out_ch\n",
        "\n",
        "        return output\n",
        "class BatchNorm1D():\n",
        "    gamma, beta = 1, 0  # 缩放因子γ和平移因子β，能训练的参数\n",
        "    def __init__(self, momentum=0.1, eps=1e-5):\n",
        "        self.running_mean = 0\n",
        "        self.running_var = 1\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        total_elem = input.numel()\n",
        "\n",
        "        if self.training:\n",
        "            # 计算均值和方差，并归一化\n",
        "            mean = input.sum() / total_elem\n",
        "            var = ((input - mean) ** 2).sum() / total_elem\n",
        "            output = (input - mean) / (var + self.eps) ** 0.5\n",
        "\n",
        "            # 更新参数\n",
        "            var_unbiased = ((input - mean) ** 2).sum() / (total_elem - 1)\n",
        "            self.running_mean = self.running_mean * (1 - self.momentum) + mean * self.momentum\n",
        "            self.running_var = self.running_var * (1 - self.momentum) + var_unbiased * self.momentum\n",
        "        else:\n",
        "            output = (input - self.running_mean) / (self.running_var + self.eps) ** 0.5\n",
        "\n",
        "        output = self.gamma * output + self.beta  # 缩放平移\n",
        "        return output"
      ],
      "metadata": {
        "id": "0mSJbG1hRS-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([\n",
        "    [\n",
        "        [1, 2, 3, 4],\n",
        "        [5, 6, 7, 8]\n",
        "    ],\n",
        "    [\n",
        "        [9, 10, 11, 12],\n",
        "        [13, 14, 15, 16]\n",
        "    ]\n",
        "])\n",
        "\n",
        "# Shape: (2, 2, 4)\n",
        "\n",
        "res1 = a.view(2, -1, 2, 2).transpose(1, 2)\n",
        "res2 = a.view(2, 2, -1, 2)\n",
        "\n",
        "print(res1)\n",
        "print(res2)\n",
        "batch_size, seq_len, d_model = Q.size()\n",
        "n_heads = 8  # 假设有8个头\n",
        "d_k = d_model // n_heads\n",
        "d_v = d_model // n_heads\n",
        "\n",
        "# 1. 投影到多个头\n",
        "WQ = torch.nn.Linear(d_model, d_model)  # 投影权重矩阵\n",
        "WK = torch.nn.Linear(d_model, d_model)\n",
        "WV = torch.nn.Linear(d_model, d_model)\n",
        "Qs = WQ(Q).view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]\n",
        "Ks = WK(K).view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)\n",
        "Vs = WV(V).view(batch_size, seq_len, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "# 2. 对每个头进行自注意力\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# d_k = Q.size(-1)  # 获取 d_k 的值\n",
        "# weights = torch.einsum('bij,bjk->bik', Q, K.transpose(-2, -1)) / (d_k**0.5)\n",
        "# normalized_weights = F.softmax(weights, dim=-1)\n",
        "# output = torch.einsum('bij,bjk->bik', normalized_weights, V)\n",
        "\n",
        "weights = torch.einsum('bhid,bhjd->bhij', Qs, Ks.transpose(-2, -1)) / (d_k ** 0.5)\n",
        "normalized_weights = F.softmax(weights, dim=-1)\n",
        "attention_output = torch.einsum('bhij,bhjd->bhid', normalized_weights, Vs)\n",
        "\n",
        "# 3. 拼接所有头的输出\n",
        "concat_attention = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "\n",
        "# 4. 通过一个线性层得到最终输出\n",
        "W_out = torch.nn.Linear(d_model, d_model)\n",
        "output = W_out(concat_attention)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmGC2stVK084",
        "outputId": "cc7f4a62-fb5b-43e6-a739-0a7bb53b35e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 1,  2],\n",
            "          [ 5,  6]],\n",
            "\n",
            "         [[ 3,  4],\n",
            "          [ 7,  8]]],\n",
            "\n",
            "\n",
            "        [[[ 9, 10],\n",
            "          [13, 14]],\n",
            "\n",
            "         [[11, 12],\n",
            "          [15, 16]]]])\n",
            "tensor([[[[ 1,  2],\n",
            "          [ 3,  4]],\n",
            "\n",
            "         [[ 5,  6],\n",
            "          [ 7,  8]]],\n",
            "\n",
            "\n",
            "        [[[ 9, 10],\n",
            "          [11, 12]],\n",
            "\n",
            "         [[13, 14],\n",
            "          [15, 16]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2psZ_iH6PZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from math import sqrt\n",
        "# import torch\n",
        "# import torch.nn\n",
        "\n",
        "\n",
        "# class Self_Attention(nn.Module):\n",
        "#     # input : batch_size * seq_len * input_dim\n",
        "#     # q : batch_size * input_dim * dim_k\n",
        "#     # k : batch_size * input_dim * dim_k\n",
        "#     # v : batch_size * input_dim * dim_v\n",
        "#     def __init__(self,input_dim,dim_k,dim_v):\n",
        "#         super(Self_Attention,self).__init__()\n",
        "#         self.q = nn.Linear(input_dim,dim_k)\n",
        "#         self.k = nn.Linear(input_dim,dim_k)\n",
        "#         self.v = nn.Linear(input_dim,dim_v)\n",
        "#         self._norm_fact = 1 / sqrt(dim_k)\n",
        "\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
        "#         K = self.k(x) # K: batch_size * seq_len * dim_k\n",
        "#         V = self.v(x) # V: batch_size * seq_len * dim_v\n",
        "\n",
        "#         atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # Q * K.T() # batch_size * seq_len * seq_len\n",
        "\n",
        "#         output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
        "\n",
        "#         return output\n",
        "# att = Self_Attention(d_model, d_qk, d_v)\n",
        "# rs2 = att(x_gen)\n",
        "# print(rs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2JIzhtNxZZM",
        "outputId": "1188c0d7-ff77-4bae-8a10-912f4691ee5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0092, -0.1427,  0.0127],\n",
            "         [-0.2397, -0.2193,  0.0599],\n",
            "         [-0.3094, -0.2240,  0.0623]]], grad_fn=<BmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "             # Encoder_input    Decoder_input        Decoder_output\n",
        "sentences = [['我 是 学 生 P' , 'S I am a student'   , 'I am a student E'],         # S: 开始符号\n",
        "             ['我 喜 欢 学 习', 'S I like learning P', 'I like learning P E'],      # E: 结束符号\n",
        "             ['我 是 男 生 P' , 'S I am a boy'       , 'I am a boy E']]             # P: 占位符号，如果当前句子不足固定长度用P占位 pad补0\n",
        "\n",
        "\n",
        "src_vocab = {'P':0, '我':1, '是':2, '学':3, '生':4, '喜':5, '欢':6,'习':7,'男':8}   # 词源字典  字：索引\n",
        "src_idx2word = {src_vocab[key]: key for key in src_vocab}\n",
        "src_vocab_size = len(src_vocab)                 # 字典字的个数\n",
        "\n",
        "tgt_vocab = {'S':0, 'E':1, 'P':2, 'I':3, 'am':4, 'a':5, 'student':6, 'like':7, 'learning':8, 'boy':9}\n",
        "idx2word = {tgt_vocab[key]: key for key in tgt_vocab}                               # 把目标字典转换成 索引：字的形式\n",
        "tgt_vocab_size = len(tgt_vocab)                                                     # 目标字典尺寸\n",
        "\n",
        "src_len = len(sentences[0][0].split(\" \"))                                           # Encoder输入的最大长度 5\n",
        "tgt_len = len(sentences[0][1].split(\" \"))                                           # Decoder输入输出最大长度 5\n",
        "\n",
        "# 把sentences 转换成字典索引\n",
        "def make_data(sentences):\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]]\n",
        "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]\n",
        "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]\n",
        "      enc_inputs.extend(enc_input)\n",
        "      dec_inputs.extend(dec_input)\n",
        "      dec_outputs.extend(dec_output)\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "print(enc_inputs)\n",
        "print(dec_inputs)\n",
        "print(dec_outputs)\n",
        "\n",
        "'''\n",
        "sentences 里一共有三个训练数据，中文->英文。把Encoder_input、Decoder_input、Decoder_output转换成字典索引，\n",
        "例如\"学\"->3、“student”->6。再把数据转换成batch大小为2的分组数据，3句话一共可以分成两组，一组2句话、一组1句话。src_len表示中文句子\n",
        "固定最大长度，tgt_len 表示英文句子固定最大长度。\n",
        "'''\n",
        "#自定义数据集函数\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self.enc_inputs = enc_inputs\n",
        "    self.dec_inputs = dec_inputs\n",
        "    self.dec_outputs = dec_outputs\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.enc_inputs.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "\n",
        "d_model = 512   # 字 Embedding 的维度\n",
        "d_ff = 2048     # 前向传播隐藏层维度\n",
        "d_k = d_v = 64  # K(=Q), V的维度\n",
        "n_layers = 6    # 有多少个encoder和decoder\n",
        "n_heads = 8     # Multi-Head Attention设置为8\n",
        "\n",
        "# 位置嵌入，position Embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,dropout=0.1,max_len=5000):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pos_table = np.array([\n",
        "        [pos / np.power(10000, 2 * i / d_model) for i in range(d_model)]\n",
        "        if pos != 0 else np.zeros(d_model) for pos in range(max_len)])\n",
        "        pos_table[1:, 0::2] = np.sin(pos_table[1:, 0::2])                  # 字嵌入维度为偶数时\n",
        "        pos_table[1:, 1::2] = np.cos(pos_table[1:, 1::2])                  # 字嵌入维度为奇数时\n",
        "        self.pos_table = torch.FloatTensor(pos_table)               # enc_inputs: [seq_len, d_model]\n",
        "    def forward(self,enc_inputs):                                         # enc_inputs: [batch_size, seq_len, d_model]\n",
        "        enc_inputs += self.pos_table[:enc_inputs.size(1),:]\n",
        "        return self.dropout(enc_inputs)\n",
        "\n",
        "'''\n",
        "Mask句子中没有实际意义的占位符，例如’我 是 学 生 P’ ，P对应句子没有实际意义，所以需要被Mask，Encoder_input 和Decoder_input占位符\n",
        "都需要被Mask。\n",
        "这就是为了处理，句子不一样长，但是输入有需要定长，不够长的pad填充，但是计算又不需要这个pad，所以mask掉\n",
        "\n",
        "这个函数最核心的一句代码是 seq_k.data.eq(0)，这句的作用是返回一个大小和 seq_k 一样的 tensor，只不过里面的值只有 True 和 False。如\n",
        "果 seq_k 某个位置的值等于 0，那么对应位置就是 True，否则即为 False。举个例子，输入为 seq_data = [1, 2, 3, 4, 0]，\n",
        "seq_data.data.eq(0) 就会返回 [False, False, False, False, True]\n",
        "'''\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()# seq_q 用于升维，为了做attention，mask score矩阵用的\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # 判断 输入那些含有P(=0),用1标记 ,[batch_size, 1, len_k]\n",
        "    return pad_attn_mask.expand(batch_size,len_q,len_k) # 扩展成多维度   [batch_size, len_q, len_k]\n",
        "\n",
        "'''\n",
        "# Decoder输入Mask\n",
        "用来Mask未来输入信息，返回的是一个上三角矩阵。比如我们在中英文翻译时候，会先把\"我是学生\"整个句子输入到Encoder中，得到最后一层的输出\n",
        "后，才会在Decoder输入\"S I am a student\"（s表示开始）,但是\"S I am a student\"这个句子我们不会一起输入，而是在T0时刻先输入\"S\"预测，\n",
        "预测第一个词\"I\"；在下一个T1时刻，同时输入\"S\"和\"I\"到Decoder预测下一个单词\"am\"；然后在T2时刻把\"S,I,am\"同时输入到Decoder预测下一个单\n",
        "词\"a\"，依次把整个句子输入到Decoder,预测出\"I am a student E\"。\n",
        "'''\n",
        "def get_attn_subsequence_mask(seq):                               # seq: [batch_size, tgt_len]\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]          # 生成上三角矩阵,[batch_size, tgt_len, tgt_len]\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()  #  [batch_size, tgt_len, tgt_len]\n",
        "    return\n",
        "\n",
        "# 计算注意力信息、残差和归一化\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):                             # Q: [batch_size, n_heads, len_q, d_k]\n",
        "                                                                       # K: [batch_size, n_heads, len_k, d_k]\n",
        "                                                                       # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "                                                                       # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)   # scores : [batch_size, n_heads, len_q, len_k]\n",
        "        scores.masked_fill_(attn_mask, -1e9)                           # 如果是停用词P就等于 0 在原tensor修改\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)                                # [batch_size, n_heads, len_q, d_v]\n",
        "        return context, attn\n",
        "\n",
        "# 多头自注意力机制\n",
        "# 拼接之后 输入fc层 加入残差 Norm\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):    # input_Q: [batch_size, len_q, d_model]\n",
        "                                                                # input_K: [batch_size, len_k, d_model]\n",
        "                                                                # input_V: [batch_size, len_v(=len_k), d_model]\n",
        "                                                                # attn_mask: [batch_size, seq_len, seq_len]\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)              # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)          # context: [batch_size, n_heads, len_q, d_v]\n",
        "                                                                                 # attn: [batch_size, n_heads, len_q, len_k]\n",
        "        # 拼接多头的结果\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
        "        output = self.fc(context)                                                # [batch_size, len_q, d_model]\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn\n",
        "\n",
        "\n",
        "'''\n",
        "## 前馈神经网络\n",
        "输入inputs ，经过两个全连接层，得到的结果再加上 inputs （残差），再做LayerNorm归一化。LayerNorm归一化可以理解层是把Batch中每一句话\n",
        "进行归一化。\n",
        "'''\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, bias=False))\n",
        "\n",
        "    def forward(self, inputs):                             # inputs: [batch_size, seq_len, d_model]\n",
        "        residual = inputs\n",
        "        output = self.fc(inputs)\n",
        "        return nn.LayerNorm(d_model)(output + residual)   # [batch_size, seq_len, d_model]\n",
        "\n",
        "## encoder layer(block)\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()                                     # 多头注意力机制\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()                                        # 前馈神经网络\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):                                # enc_inputs: [batch_size, src_len, d_model]\n",
        "        #输入3个enc_inputs分别与W_q、W_k、W_v相乘得到Q、K、V                             # enc_self_attn_mask: [batch_size, src_len, src_len]\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,    # enc_outputs: [batch_size, src_len, d_model],\n",
        "                                               enc_self_attn_mask)                    # attn: [batch_size, n_heads, src_len, src_len]\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)                                       # enc_outputs: [batch_size, src_len, d_model]\n",
        "        return enc_outputs, attn\n",
        "\n",
        "'''\n",
        "## Encoder\n",
        "第一步，中文字索引进行Embedding，转换成512维度的字向量。\n",
        "第二步，在子向量上面加上位置信息。\n",
        "第三步，Mask掉句子中的占位符号。\n",
        "第四步，通过6层的encoder（上一层的输出作为下一层的输入）。\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [EncoderLayer() for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, enc_inputs):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        '''\n",
        "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers:\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        return enc_outputs, enc_self_attns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 测试\n",
        "'''\n",
        "enc_inputs: 每个单词在字典当中的位置。下面是三句话，每个数字代表：该位置上的单词在字典中的位置。\n",
        "tensor([[1, 2, 3, 4, 0],\n",
        "        [1, 5, 6, 3, 7],\n",
        "        [1, 2, 8, 4, 0]])\n",
        "'''\n",
        "enc_outputs, enc_self_attns = Encoder()(enc_inputs)\n",
        "print(enc_outputs.shape)    # torch.Size([3, 5, 512])\n"
      ],
      "metadata": {
        "id": "rR4i8Csw6Gdm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}